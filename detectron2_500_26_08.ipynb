{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detectron2_500_26.08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOk+QvPia8Uy6EzDAMngm7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidyabandgar97/A2A/blob/main/detectron2_500_26_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bMVIPsfx2J8",
        "outputId": "b94837d7-1dd6-4298-d26d-21f0cc58facd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqLM6ZDXyK4k",
        "outputId": "dc41c182-6851-4e63-f5bc-131335b18916"
      },
      "source": [
        "%cd /content/drive/MyDrive/detectron2_500/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/detectron2_500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxkNSxrLyLRE",
        "outputId": "653f860d-94ce-4e27-9807-601e7960772c"
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Collecting torch==1.5\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8 MB 23 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 19.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.5.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 12.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=90ecffcff9a2ba51b2d18d0a903dc0a418b919ef240a8db068e59203544fe204\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-8_wuxmzb\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-8_wuxmzb\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.24)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=263926 sha256=444407d7259e76961cb920f14d8389ceef68b24c8b45e9644f34fda17debd21e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2cbg6yic/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.2\n",
            "    Uninstalling pycocotools-2.0.2:\n",
            "      Successfully uninstalled pycocotools-2.0.2\n",
            "Successfully installed pycocotools-2.0\n",
            "1.5.0+cu101 True\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_PZPk-YyLXb",
        "outputId": "afebcd09-eb4f-4027-9851-75eade6c7f84"
      },
      "source": [
        "!pip install torch==1.9.0 torchvision==0.4.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 1.7 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.1\n",
            "  Downloading torchvision-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.1) (1.15.0)\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==1.9.0 and torchvision==0.4.1 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested torch==1.9.0\n",
            "    torchvision 0.4.1 depends on torch==1.3.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ0DdTcCyLd3",
        "outputId": "5df9b0ad-46b2-49ec-bafa-b13c5257dd08"
      },
      "source": [
        "!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html\n",
            "Collecting detectron2==0.1.3\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/detectron2-0.1.3%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 743 kB/s \n",
            "\u001b[?25hCollecting fvcore>=0.1.1\n",
            "  Downloading fvcore-0.1.5.post20210825.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (3.2.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (4.62.0)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (7.1.2)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.1.3) (0.8.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore>=0.1.1->detectron2==0.1.3) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore>=0.1.1->detectron2==0.1.3) (5.1)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.1.3) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->detectron2==0.1.3) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.4.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.34.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.39.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.1.3) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.3) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.3) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.1.3) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.3) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->detectron2==0.1.3) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->detectron2==0.1.3) (3.7.4.3)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20210825-py3-none-any.whl size=60661 sha256=0fcb7e9d47b061d8b4341f7d5363f9daa72875aa54fbfa34aaa9207bd8cbc6bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/c4/f8/c4cb07f135845218b019b4a55d8a0470a0f21ee13f8dcd16be\n",
            "Successfully built fvcore\n",
            "Installing collected packages: portalocker, yacs, iopath, mock, fvcore, detectron2\n",
            "Successfully installed detectron2-0.1.3+cu101 fvcore-0.1.5.post20210825 iopath-0.1.9 mock-4.0.3 portalocker-2.3.1 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHgEQepByLiE",
        "outputId": "eab0eb06-00e7-41ca-8926-23c3e0d248e7"
      },
      "source": [
        "# check pytorch installation: \n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "#assert torch.__version__.startswith(\"1.9\")   # please manually install torch 1.9 if Colab changes its default version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RQjg1RtyLlF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ekF8oZVN6Pl"
      },
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset_train2\", {}, \"/content/drive/MyDrive/detectron2_500/train.json\", \"/content/drive/MyDrive/detectron2_500/Dataset\")\n",
        "#register_coco_instances(\"my_dataset_val\", {}, \"/content/drive/MyDrive/detectron_train/test/test.json\", \"/content/drive/MyDrive/detectron_train/test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kKjvVB_rN6RT",
        "outputId": "b47e851c-0718-4c4c-9337-ba322d8c5e58"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "import os\n",
        "\n",
        "#!mkdir /content/drive/MyDrive/18_Aug_Dataset\n",
        "#!cp -r /content/Dataset /content/drive/MyDrive/18_Aug_Dataset\n",
        "#!cp /content/main.json /content/drive/MyDrive/18_Aug_Dataset\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = \"/content/drive/MyDrive/detectron2_500/output_detect\"\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 100\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train2\",)\n",
        "#cfg.DATASETS.TEST = (\"my_dataset_train\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 10000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=True)\n",
        "trainer.train()\n",
        "trainer.test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[08/26 11:17:47 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): DeformBottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(128, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(128, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(128, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(128, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): DeformBottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): DeformBottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): DeformBottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2_offset): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (conv2): DeformConv(\n",
            "            in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), groups=1, deformable_groups=1, bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (predictor): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[08/26 11:17:48 d2.data.datasets.coco]: \u001b[0mLoaded 689 images in COCO format from /content/drive/MyDrive/detectron2_500/train.json\n",
            "\u001b[32m[08/26 11:17:48 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 689 images left.\n",
            "\u001b[32m[08/26 11:17:48 d2.data.build]: \u001b[0mDistribution of instances among all 6 categories:\n",
            "\u001b[36m|  category  | #instances   |  category  | #instances   |   category    | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:-------------:|:-------------|\n",
            "| aeroplane  | 143          |    car     | 500          |     chair     | 500          |\n",
            "|    cow     | 372          |   person   | 500          | traffic_light | 499          |\n",
            "|            |              |            |              |               |              |\n",
            "|   total    | 2514         |            |              |               |              |\u001b[0m\n",
            "\u001b[32m[08/26 11:17:48 d2.data.common]: \u001b[0mSerializing 689 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[08/26 11:17:48 d2.data.common]: \u001b[0mSerialized dataset takes 1.35 MiB\n",
            "\u001b[32m[08/26 11:17:48 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[08/26 11:17:48 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[08/26 11:18:00 d2.engine.train_loop]: \u001b[0mStarting training from iteration 7700\n",
            "\u001b[32m[08/26 11:18:36 d2.utils.events]: \u001b[0m eta: 1:09:03  iter: 7719  total_loss: 0.373  loss_cls: 0.053  loss_box_reg: 0.167  loss_mask: 0.162  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  time: 1.7890  data_time: 0.0629  lr: 0.000250  max_mem: 2571M\n",
            "\u001b[32m[08/26 11:19:13 d2.utils.events]: \u001b[0m eta: 1:08:57  iter: 7739  total_loss: 0.501  loss_cls: 0.076  loss_box_reg: 0.205  loss_mask: 0.201  loss_rpn_cls: 0.005  loss_rpn_loc: 0.010  time: 1.7995  data_time: 0.0078  lr: 0.000250  max_mem: 2572M\n",
            "\u001b[32m[08/26 11:19:48 d2.utils.events]: \u001b[0m eta: 1:07:36  iter: 7759  total_loss: 0.568  loss_cls: 0.104  loss_box_reg: 0.186  loss_mask: 0.179  loss_rpn_cls: 0.005  loss_rpn_loc: 0.022  time: 1.7897  data_time: 0.0073  lr: 0.000250  max_mem: 2676M\n",
            "\u001b[32m[08/26 11:20:25 d2.utils.events]: \u001b[0m eta: 1:07:09  iter: 7779  total_loss: 0.600  loss_cls: 0.100  loss_box_reg: 0.254  loss_mask: 0.216  loss_rpn_cls: 0.007  loss_rpn_loc: 0.021  time: 1.7999  data_time: 0.0080  lr: 0.000250  max_mem: 2692M\n",
            "\u001b[32m[08/26 11:21:06 d2.utils.events]: \u001b[0m eta: 1:06:49  iter: 7799  total_loss: 0.510  loss_cls: 0.075  loss_box_reg: 0.208  loss_mask: 0.181  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.8216  data_time: 0.0073  lr: 0.000250  max_mem: 2692M\n",
            "\u001b[32m[08/26 11:21:42 d2.utils.events]: \u001b[0m eta: 1:06:42  iter: 7819  total_loss: 0.427  loss_cls: 0.087  loss_box_reg: 0.167  loss_mask: 0.174  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.8220  data_time: 0.0072  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:22:17 d2.utils.events]: \u001b[0m eta: 1:05:38  iter: 7839  total_loss: 0.454  loss_cls: 0.070  loss_box_reg: 0.198  loss_mask: 0.182  loss_rpn_cls: 0.003  loss_rpn_loc: 0.015  time: 1.8134  data_time: 0.0072  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:22:54 d2.utils.events]: \u001b[0m eta: 1:05:06  iter: 7859  total_loss: 0.484  loss_cls: 0.071  loss_box_reg: 0.223  loss_mask: 0.171  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.8149  data_time: 0.0080  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:23:29 d2.utils.events]: \u001b[0m eta: 1:04:19  iter: 7879  total_loss: 0.417  loss_cls: 0.066  loss_box_reg: 0.191  loss_mask: 0.151  loss_rpn_cls: 0.001  loss_rpn_loc: 0.012  time: 1.8106  data_time: 0.0071  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:24:08 d2.utils.events]: \u001b[0m eta: 1:03:42  iter: 7899  total_loss: 0.455  loss_cls: 0.072  loss_box_reg: 0.192  loss_mask: 0.169  loss_rpn_cls: 0.001  loss_rpn_loc: 0.008  time: 1.8095  data_time: 0.0075  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:24:42 d2.utils.events]: \u001b[0m eta: 1:03:05  iter: 7919  total_loss: 0.602  loss_cls: 0.112  loss_box_reg: 0.233  loss_mask: 0.213  loss_rpn_cls: 0.006  loss_rpn_loc: 0.024  time: 1.8024  data_time: 0.0074  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:25:20 d2.utils.events]: \u001b[0m eta: 1:02:29  iter: 7939  total_loss: 0.475  loss_cls: 0.078  loss_box_reg: 0.197  loss_mask: 0.195  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.8073  data_time: 0.0070  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:25:56 d2.utils.events]: \u001b[0m eta: 1:01:52  iter: 7959  total_loss: 0.534  loss_cls: 0.064  loss_box_reg: 0.205  loss_mask: 0.174  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.8093  data_time: 0.0073  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:26:31 d2.utils.events]: \u001b[0m eta: 1:01:06  iter: 7979  total_loss: 0.473  loss_cls: 0.064  loss_box_reg: 0.240  loss_mask: 0.202  loss_rpn_cls: 0.007  loss_rpn_loc: 0.011  time: 1.8024  data_time: 0.0075  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:27:09 d2.utils.events]: \u001b[0m eta: 1:00:28  iter: 7999  total_loss: 0.452  loss_cls: 0.062  loss_box_reg: 0.203  loss_mask: 0.179  loss_rpn_cls: 0.003  loss_rpn_loc: 0.014  time: 1.8019  data_time: 0.0070  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:27:45 d2.utils.events]: \u001b[0m eta: 0:59:53  iter: 8019  total_loss: 0.541  loss_cls: 0.102  loss_box_reg: 0.232  loss_mask: 0.192  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.8028  data_time: 0.0083  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:28:21 d2.utils.events]: \u001b[0m eta: 0:59:16  iter: 8039  total_loss: 0.462  loss_cls: 0.080  loss_box_reg: 0.184  loss_mask: 0.178  loss_rpn_cls: 0.006  loss_rpn_loc: 0.021  time: 1.8024  data_time: 0.0070  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:28:56 d2.utils.events]: \u001b[0m eta: 0:58:35  iter: 8059  total_loss: 0.460  loss_cls: 0.101  loss_box_reg: 0.203  loss_mask: 0.165  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.7998  data_time: 0.0089  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:29:32 d2.utils.events]: \u001b[0m eta: 0:57:57  iter: 8079  total_loss: 0.456  loss_cls: 0.067  loss_box_reg: 0.190  loss_mask: 0.171  loss_rpn_cls: 0.003  loss_rpn_loc: 0.014  time: 1.7999  data_time: 0.0105  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:30:09 d2.utils.events]: \u001b[0m eta: 0:57:19  iter: 8099  total_loss: 0.573  loss_cls: 0.087  loss_box_reg: 0.240  loss_mask: 0.192  loss_rpn_cls: 0.006  loss_rpn_loc: 0.030  time: 1.7973  data_time: 0.0084  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:30:46 d2.utils.events]: \u001b[0m eta: 0:56:43  iter: 8119  total_loss: 0.448  loss_cls: 0.068  loss_box_reg: 0.196  loss_mask: 0.168  loss_rpn_cls: 0.001  loss_rpn_loc: 0.009  time: 1.7979  data_time: 0.0085  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:31:22 d2.utils.events]: \u001b[0m eta: 0:56:07  iter: 8139  total_loss: 0.407  loss_cls: 0.056  loss_box_reg: 0.138  loss_mask: 0.172  loss_rpn_cls: 0.002  loss_rpn_loc: 0.007  time: 1.7987  data_time: 0.0070  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:31:58 d2.utils.events]: \u001b[0m eta: 0:55:31  iter: 8159  total_loss: 0.436  loss_cls: 0.076  loss_box_reg: 0.177  loss_mask: 0.166  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  time: 1.7992  data_time: 0.0105  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:32:35 d2.utils.events]: \u001b[0m eta: 0:54:58  iter: 8179  total_loss: 0.583  loss_cls: 0.080  loss_box_reg: 0.221  loss_mask: 0.203  loss_rpn_cls: 0.001  loss_rpn_loc: 0.011  time: 1.8004  data_time: 0.0099  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:33:12 d2.utils.events]: \u001b[0m eta: 0:54:18  iter: 8199  total_loss: 0.465  loss_cls: 0.092  loss_box_reg: 0.175  loss_mask: 0.180  loss_rpn_cls: 0.003  loss_rpn_loc: 0.017  time: 1.7981  data_time: 0.0084  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:33:48 d2.utils.events]: \u001b[0m eta: 0:53:42  iter: 8219  total_loss: 0.439  loss_cls: 0.062  loss_box_reg: 0.206  loss_mask: 0.182  loss_rpn_cls: 0.001  loss_rpn_loc: 0.011  time: 1.7981  data_time: 0.0075  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:34:21 d2.utils.events]: \u001b[0m eta: 0:53:04  iter: 8239  total_loss: 0.402  loss_cls: 0.076  loss_box_reg: 0.143  loss_mask: 0.172  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.7940  data_time: 0.0092  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:34:56 d2.utils.events]: \u001b[0m eta: 0:52:23  iter: 8259  total_loss: 0.537  loss_cls: 0.104  loss_box_reg: 0.214  loss_mask: 0.177  loss_rpn_cls: 0.006  loss_rpn_loc: 0.017  time: 1.7925  data_time: 0.0080  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:35:32 d2.utils.events]: \u001b[0m eta: 0:51:43  iter: 8279  total_loss: 0.523  loss_cls: 0.105  loss_box_reg: 0.188  loss_mask: 0.185  loss_rpn_cls: 0.003  loss_rpn_loc: 0.019  time: 1.7926  data_time: 0.0080  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:36:11 d2.utils.events]: \u001b[0m eta: 0:51:11  iter: 8299  total_loss: 0.563  loss_cls: 0.098  loss_box_reg: 0.255  loss_mask: 0.195  loss_rpn_cls: 0.003  loss_rpn_loc: 0.017  time: 1.7932  data_time: 0.0070  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:36:49 d2.utils.events]: \u001b[0m eta: 0:50:40  iter: 8319  total_loss: 0.603  loss_cls: 0.122  loss_box_reg: 0.235  loss_mask: 0.186  loss_rpn_cls: 0.008  loss_rpn_loc: 0.027  time: 1.7964  data_time: 0.0084  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:37:25 d2.utils.events]: \u001b[0m eta: 0:50:05  iter: 8339  total_loss: 0.395  loss_cls: 0.055  loss_box_reg: 0.155  loss_mask: 0.159  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  time: 1.7965  data_time: 0.0088  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:38:01 d2.utils.events]: \u001b[0m eta: 0:49:28  iter: 8359  total_loss: 0.630  loss_cls: 0.102  loss_box_reg: 0.226  loss_mask: 0.202  loss_rpn_cls: 0.005  loss_rpn_loc: 0.021  time: 1.7968  data_time: 0.0073  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:38:38 d2.utils.events]: \u001b[0m eta: 0:48:53  iter: 8379  total_loss: 0.509  loss_cls: 0.081  loss_box_reg: 0.203  loss_mask: 0.207  loss_rpn_cls: 0.006  loss_rpn_loc: 0.014  time: 1.7983  data_time: 0.0097  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:39:16 d2.utils.events]: \u001b[0m eta: 0:48:16  iter: 8399  total_loss: 0.523  loss_cls: 0.099  loss_box_reg: 0.259  loss_mask: 0.181  loss_rpn_cls: 0.004  loss_rpn_loc: 0.014  time: 1.7975  data_time: 0.0113  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:39:51 d2.utils.events]: \u001b[0m eta: 0:47:39  iter: 8419  total_loss: 0.534  loss_cls: 0.078  loss_box_reg: 0.232  loss_mask: 0.211  loss_rpn_cls: 0.005  loss_rpn_loc: 0.016  time: 1.7968  data_time: 0.0106  lr: 0.000250  max_mem: 2781M\n",
            "\u001b[32m[08/26 11:39:56 d2.engine.hooks]: \u001b[0mOverall training speed: 720 iterations in 0:21:35 (1.7987 s / it)\n",
            "\u001b[32m[08/26 11:39:56 d2.engine.hooks]: \u001b[0mTotal training time: 0:21:52 (0:00:17 on hooks)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-c277a2bb3066>\", line 30, in <module>\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\", line 401, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\", line 132, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\", line 215, in run_step\n",
            "    loss_dict = self.model(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/rcnn.py\", line 130, in forward\n",
            "    _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\", line 669, in forward\n",
            "    losses.update(self._forward_mask(features, proposals))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/roi_heads/roi_heads.py\", line 773, in _forward_mask\n",
            "    return self.mask_head(mask_features, proposals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/roi_heads/mask_head.py\", line 182, in forward\n",
            "    return {\"loss_mask\": mask_rcnn_loss(x, instances, self.vis_period)}\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/roi_heads/mask_head.py\", line 56, in mask_rcnn_loss\n",
            "    instances_per_image.proposal_boxes.tensor, mask_side_len\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/detectron2/structures/masks.py\", line 373, in crop_and_resize\n",
            "    boxes = boxes.to(torch.device(\"cpu\"))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvxqNBfzN6VM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AL7ToXkN6X3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N7afcQ4N6am"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhzBeVimN6c-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8wKoYSaN6gQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ2eiLD1N6jc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}